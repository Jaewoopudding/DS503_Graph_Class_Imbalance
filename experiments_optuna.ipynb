{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.datasets import CoraFull, Planetoid, CitationFull\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "from models import GAT, GraphSAGE, GIN\n",
    "from utils import train_model, test_model, train_constrative_model, valid_model\n",
    "from mean_average_distance import MAD, MADGap\n",
    "from virtualnode import VirtualClassNode_init, UnidirectionalVirtualClassNode_init, VirtualClassNode, UnidirectionalVirtualClassNode\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH = 'results'\n",
    "EARLY_STOPPING = 50\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = CitationFull(root='dataset/Cora', name='Cora', transform=NormalizeFeatures())\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19793, 8710], edge_index=[2, 126842], y=[19793], train_mask=[19793], valid_mask=[19793], test_mask=[19793])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "df = pd.DataFrame(data.x)\n",
    "df['y'] = data.y\n",
    "train, valid = train_test_split(df, stratify=df.y, test_size=0.4)\n",
    "valid, test = train_test_split(valid, stratify=valid.y, test_size=0.5)\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train.index]=True\n",
    "data.valid_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.valid_mask[valid.index]=True\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask[test.index]=True\n",
    "data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad = MAD(device=device, global_flag=True)\n",
    "madgap = MADGap(device, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = torchmetrics.F1Score(task = \"multiclass\", average=\"micro\", num_classes=dataset.num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_result = pd.DataFrame({\n",
    "                            'trial' : [],\n",
    "                            'model' : [],\n",
    "                            'virtualnode' : [],\n",
    "                            'vnode_init' : [],\n",
    "                            'temperature' : [],\n",
    "                            'constrative coef' : [],\n",
    "                            'gaussian_noise_scale' : [],\n",
    "                            'lr' : [],\n",
    "                            'train_acc' : [],\n",
    "                            'train_loss' : [],\n",
    "                            'val_acc' : [],\n",
    "                            'val_loss' : [],\n",
    "                            'test_acc' : [],\n",
    "                            'macro f1' : [],\n",
    "                            'micro f1'\n",
    "                            'minor f1' : [],\n",
    "                            'mad' : [],\n",
    "                            'madgap' : []                            \n",
    "                            })\n",
    "\n",
    "tuning_result.to_csv(\"training_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    Vnodes = trial.suggest_categorical(\"vnode\", [\"uni\", \"bi\"])\n",
    "\n",
    "    # vnode_init = trial.suggest_categorical(\"class_mean\", [\"random\", \"zero\"])\n",
    "    \n",
    "    if Vnodes == \"uni\":\n",
    "        vc = UnidirectionalVirtualClassNode()\n",
    "        data_for_tuning = vc.forward(data)\n",
    "    else:\n",
    "        vc = VirtualClassNode()\n",
    "        data_for_tuning = vc.forward(data)\n",
    "        \n",
    "    temperature = trial.suggest_float(\"temperature\", 0.1, 10)\n",
    "    constrative_coef = trial.suggest_loguniform(\"contrastive_loss_coef\", 1e-5, 1)\n",
    "    cnode_weight = trial.suggest_loguniform(\"cnode_weight\", 1, 10)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 5e-4, 1e-2)\n",
    "    positive_sampling = trial.suggest_categorical(\"positive_sampling\", [True, False])\n",
    "\n",
    "    g_noise = trial.suggest_categorical(\"g_noise\", [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "\n",
    "    #model_ = trial.suggest_categorical(\"model\", [\"sage\", \"gat\", \"gin\"])\n",
    "    #if model_ == \"sage\":\n",
    "    model = GraphSAGE(in_channels=dataset.num_features, hidden_channels=256, number_of_classes=dataset.num_classes, num_of_hidden_layers=4, device=device, noise_level=g_noise)\n",
    "    #elif model_ == \"gat\":\n",
    "    #    model = GAT(in_channels=dataset.num_features, hidden_channels=476, number_of_classes=dataset.num_classes, num_of_hidden_layers=4, device=device, heads=1, noise_level=g_noise)\n",
    "    #elif model_ == \"gin\":\n",
    "    #    model = GIN(in_channels=dataset.num_features, hidden_channels=415, number_of_classes=dataset.num_classes, num_of_hidden_layers=4, device=device, noise_level=g_noise)\n",
    "    print('='*110)\n",
    "    print(f'VC : {Vnodes}, lr : {lr:.5f}, temp : {temperature:.5f}, constrative coef : {constrative_coef:.5f}')\n",
    "    print(f'noise : {g_noise:.4f}, cnode weight: {cnode_weight:.2f}, positive_sampling: {positive_sampling} ')\n",
    "    max_loss = 10000\n",
    "    early_stopping_count = 0\n",
    "    print(f'Model: {model.name} | Number of parameters: {model.get_n_params()}')\n",
    "    print('')\n",
    "    model = model.to(device)\n",
    "    data_for_tuning = data_for_tuning.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    for epoch in range(2000):\n",
    "        loss, acc = train_constrative_model(model, data_for_tuning, optimizer, criterion, \n",
    "                                            constrative_coef=constrative_coef, \n",
    "                                            temperature=temperature, \n",
    "                                            cnode_weight=cnode_weight,\n",
    "                                            positive_sampling=positive_sampling)\n",
    "        losses.append(loss.item())\n",
    "        accs.append(100*acc)\n",
    "        val_loss, val_acc = valid_model(model, data_for_tuning, criterion, \n",
    "                                        constrative_coef=constrative_coef, temperature=temperature, \n",
    "                                        cnode_weight=cnode_weight,\n",
    "                                        positive_sampling = positive_sampling)\n",
    "        val_accs.append(100*val_acc)\n",
    "        if val_loss < max_loss:\n",
    "            max_loss = val_loss\n",
    "            early_stopping_count = 0\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            if early_stopping_count > EARLY_STOPPING:\n",
    "                print(\"Early stopping..\")\n",
    "                break\n",
    "        if epoch%20==0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train Acc: {100*acc:.2f}, Valid Loss: {val_loss:.4f}, Valid Acc: {100*val_acc:.2f}')\n",
    "        if epoch > 1000:\n",
    "            if val_acc < 0.4:\n",
    "                print('underfitting...')\n",
    "                break\n",
    "    report = test_model(model, data_for_tuning)\n",
    "    result = pd.DataFrame(report).T\n",
    "    result_sliced = result.iloc[:-3 if len(result) < 23 else 20, :]\n",
    "    test_acc = result.loc['accuracy'][0]\n",
    "    result.loc['minorities-f1',:] = result_sliced.mean(axis=0)\n",
    "    result.to_csv(os.path.join(SAVE_PATH, f'{model.name}_layers{model.num_of_hidden_layers}_neurons{model.hidden_channels}'+'.csv'))\n",
    "    result = model(data_for_tuning.x.to(device), data_for_tuning.edge_index.to(device))[0].cpu()\n",
    "    global_mad = mad(result).item()\n",
    "    mad_gap = madgap(result, data_for_tuning.edge_index).item()\n",
    "\n",
    "    model.eval()\n",
    "    out, _ = model(data_for_tuning.x, data_for_tuning.edge_index)\n",
    "    pred = out.argmax(dim=-1)\n",
    "    f1 = f1score(data_for_tuning.y[data_for_tuning.valid_mask], pred[data_for_tuning.valid_mask])\n",
    "\n",
    "    exp_result_dict = {\n",
    "        'trial' : trial.number,\n",
    "        'model' : model.name,\n",
    "        'virtualnode' : vc,\n",
    "        #'vnode_init' : vnode_init,\n",
    "        'temperature' : temperature,\n",
    "        'constrative coef' : constrative_coef,\n",
    "        'gaussian_noise_scale'  : g_noise,\n",
    "        'lr' : lr,\n",
    "        'train_acc' : acc,\n",
    "        'train_loss' : loss,\n",
    "        'val_acc' : val_acc,\n",
    "        'val_loss' : val_loss,\n",
    "        'test_acc' : test_acc,\n",
    "        'macro f1' : pd.DataFrame(report).T.loc['macro avg', 'f1-score'],\n",
    "        'micro f1' : pd.DataFrame(report).T.loc['weighted avg', 'f1-score'],\n",
    "        'minor f1' : pd.DataFrame(report).T[:-3].sort_values(by='support', ascending=False)[-11:].mean()['f1-score'],\n",
    "        'mad' : global_mad,\n",
    "        'madgap' : mad_gap                            \n",
    "    }\n",
    "    \n",
    "    \n",
    "    tuning_result = pd.read_csv(\"training_res.csv\")\n",
    "    exp_result_dict = pd.DataFrame(exp_result_dict, index=[trial.number])\n",
    "    pd.concat([tuning_result, exp_result_dict], axis=0).to_csv(\"training_res.csv\", index=False)\n",
    "    \n",
    "    torch.save(model.state_dict() , \"model_file/{}_{}.pt\".format(model.name, str(trial.number)))\n",
    "                    \n",
    "    print(f'global_mad: {global_mad}')\n",
    "    print(f'madgap: {mad_gap}')\n",
    "    print(f'Test Acc: {100*test_acc}')\n",
    "    \n",
    "    print('==========================================', end='\\n\\n')\n",
    "\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-06 18:48:28,623]\u001b[0m A new study created in memory with name: no-name-5fd6dbc3-8539-474c-a3b6-8b255d06bb25\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "VC : bi, lr : 0.00089, temp : 5.29141, constrative coef : 0.00046\n",
      "noise : 0.0000, cnode weight: 1.86, positive_sampling: True \n",
      "Model: Graph Attention Network | Number of parameters: 4831400\n",
      "\n",
      "Epoch: 000, Train Loss: 7.9060, Train Acc: 0.24, Valid Loss: 7.9071, Valid Acc: 1.54\n",
      "Epoch: 020, Train Loss: 7.4364, Train Acc: 10.15, Valid Loss: 7.3172, Valid Acc: 11.44\n",
      "Epoch: 040, Train Loss: 4.8994, Train Acc: 28.89, Valid Loss: 4.6785, Valid Acc: 26.98\n",
      "Epoch: 060, Train Loss: 2.0373, Train Acc: 58.67, Valid Loss: 1.8896, Valid Acc: 46.74\n",
      "Epoch: 080, Train Loss: 1.3994, Train Acc: 69.30, Valid Loss: 1.2650, Valid Acc: 53.64\n",
      "Epoch: 100, Train Loss: 1.1493, Train Acc: 73.09, Valid Loss: 1.0380, Valid Acc: 55.94\n",
      "Epoch: 120, Train Loss: 0.9521, Train Acc: 76.69, Valid Loss: 0.8383, Valid Acc: 58.03\n",
      "Epoch: 140, Train Loss: 0.7653, Train Acc: 81.26, Valid Loss: 0.6909, Valid Acc: 57.86\n",
      "Epoch: 160, Train Loss: 0.6805, Train Acc: 83.38, Valid Loss: 0.5983, Valid Acc: 55.85\n",
      "Epoch: 180, Train Loss: 0.5994, Train Acc: 84.68, Valid Loss: 0.4996, Valid Acc: 55.80\n",
      "Epoch: 200, Train Loss: 0.4846, Train Acc: 87.49, Valid Loss: 0.3960, Valid Acc: 54.06\n",
      "Epoch: 220, Train Loss: 0.5266, Train Acc: 87.46, Valid Loss: 0.3646, Valid Acc: 50.38\n",
      "Epoch: 240, Train Loss: 0.3577, Train Acc: 90.78, Valid Loss: 0.2950, Valid Acc: 50.86\n",
      "Epoch: 260, Train Loss: 0.4348, Train Acc: 88.56, Valid Loss: 0.3255, Valid Acc: 50.43\n",
      "Epoch: 280, Train Loss: 0.3441, Train Acc: 90.95, Valid Loss: 0.2509, Valid Acc: 46.12\n",
      "Epoch: 300, Train Loss: 0.2865, Train Acc: 92.16, Valid Loss: 0.2287, Valid Acc: 45.10\n",
      "Epoch: 320, Train Loss: 0.3231, Train Acc: 93.24, Valid Loss: 0.1965, Valid Acc: 45.62\n",
      "Epoch: 340, Train Loss: 0.2554, Train Acc: 92.98, Valid Loss: 0.1803, Valid Acc: 45.42\n",
      "Epoch: 360, Train Loss: 0.3242, Train Acc: 91.44, Valid Loss: 0.1721, Valid Acc: 41.90\n",
      "Epoch: 380, Train Loss: 0.2517, Train Acc: 93.13, Valid Loss: 0.1937, Valid Acc: 41.40\n",
      "Epoch: 400, Train Loss: 0.2955, Train Acc: 92.44, Valid Loss: 0.1121, Valid Acc: 43.09\n",
      "Epoch: 420, Train Loss: 0.1551, Train Acc: 96.22, Valid Loss: 0.0849, Valid Acc: 42.94\n",
      "Epoch: 440, Train Loss: 0.1359, Train Acc: 96.60, Valid Loss: 0.0686, Valid Acc: 42.19\n",
      "Epoch: 460, Train Loss: 0.1097, Train Acc: 97.61, Valid Loss: 0.0506, Valid Acc: 40.66\n",
      "Epoch: 480, Train Loss: 0.1529, Train Acc: 96.79, Valid Loss: 0.0595, Valid Acc: 41.03\n",
      "Epoch: 500, Train Loss: 0.0975, Train Acc: 97.93, Valid Loss: 0.0373, Valid Acc: 41.33\n",
      "Epoch: 520, Train Loss: 0.1132, Train Acc: 97.68, Valid Loss: 0.0297, Valid Acc: 38.92\n",
      "Epoch: 540, Train Loss: 0.1524, Train Acc: 96.41, Valid Loss: 0.0210, Valid Acc: 39.07\n",
      "Epoch: 560, Train Loss: 0.1084, Train Acc: 97.56, Valid Loss: 0.0266, Valid Acc: 36.19\n",
      "Epoch: 580, Train Loss: 0.0513, Train Acc: 98.88, Valid Loss: 0.0213, Valid Acc: 37.35\n",
      "Epoch: 600, Train Loss: 0.0623, Train Acc: 98.69, Valid Loss: 0.0173, Valid Acc: 36.63\n",
      "Epoch: 620, Train Loss: 0.0491, Train Acc: 98.86, Valid Loss: 0.0130, Valid Acc: 35.19\n",
      "Epoch: 640, Train Loss: 0.0251, Train Acc: 99.53, Valid Loss: 0.0101, Valid Acc: 35.64\n",
      "Epoch: 660, Train Loss: 0.0367, Train Acc: 99.36, Valid Loss: 0.0071, Valid Acc: 35.67\n",
      "Epoch: 680, Train Loss: 0.0359, Train Acc: 99.39, Valid Loss: 0.0085, Valid Acc: 35.47\n",
      "Epoch: 700, Train Loss: 0.0329, Train Acc: 99.27, Valid Loss: 0.0107, Valid Acc: 33.13\n",
      "Early stopping..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-06 18:50:04,059]\u001b[0m Trial 0 finished with value: 0.3509555757045746 and parameters: {'vnode': 'bi', 'temperature': 5.291411379945298, 'contrastive_loss_coef': 0.0004563739956089729, 'cnode_weight': 1.8602085144439549, 'lr': 0.0008851974040639738, 'positive_sampling': True, 'g_noise': 1e-06, 'model': 'gat'}. Best is trial 0 with value: 0.3509555757045746.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_mad: 0.7962252497673035\n",
      "madgap: 0.45841795206069946\n",
      "Test Acc: 33.392270775448345\n",
      "==========================================\n",
      "\n",
      "==============================================================================================================\n",
      "VC : bi, lr : 0.00303, temp : 2.57437, constrative coef : 0.00003\n",
      "noise : 0.0000, cnode weight: 4.01, positive_sampling: True \n",
      "Model: GraphSAGE | Number of parameters: 4853760\n",
      "\n",
      "Epoch: 000, Train Loss: 17.0421, Train Acc: 1.00, Valid Loss: 17.0128, Valid Acc: 3.92\n",
      "Epoch: 020, Train Loss: 8.6815, Train Acc: 28.20, Valid Loss: 7.7659, Valid Acc: 28.42\n",
      "Epoch: 040, Train Loss: 1.9147, Train Acc: 63.93, Valid Loss: 1.6203, Valid Acc: 53.36\n",
      "Epoch: 060, Train Loss: 0.9518, Train Acc: 77.21, Valid Loss: 0.7862, Valid Acc: 62.60\n",
      "Epoch: 080, Train Loss: 0.6461, Train Acc: 83.29, Valid Loss: 0.5306, Valid Acc: 65.10\n",
      "Epoch: 100, Train Loss: 0.5068, Train Acc: 85.85, Valid Loss: 0.4008, Valid Acc: 66.07\n",
      "Epoch: 120, Train Loss: 0.4141, Train Acc: 87.72, Valid Loss: 0.3035, Valid Acc: 67.46\n",
      "Epoch: 140, Train Loss: 0.3500, Train Acc: 89.40, Valid Loss: 0.2342, Valid Acc: 68.21\n",
      "Epoch: 160, Train Loss: 0.3023, Train Acc: 90.23, Valid Loss: 0.1850, Valid Acc: 68.50\n",
      "Epoch: 180, Train Loss: 0.2489, Train Acc: 91.97, Valid Loss: 0.1431, Valid Acc: 68.50\n",
      "Epoch: 200, Train Loss: 0.2298, Train Acc: 92.42, Valid Loss: 0.1229, Valid Acc: 68.35\n",
      "Epoch: 220, Train Loss: 0.2038, Train Acc: 93.37, Valid Loss: 0.0920, Valid Acc: 68.38\n",
      "Epoch: 240, Train Loss: 0.1750, Train Acc: 94.42, Valid Loss: 0.0774, Valid Acc: 68.70\n",
      "Epoch: 260, Train Loss: 0.1596, Train Acc: 94.68, Valid Loss: 0.0606, Valid Acc: 68.13\n",
      "Epoch: 280, Train Loss: 0.1452, Train Acc: 95.14, Valid Loss: 0.0568, Valid Acc: 68.68\n",
      "Epoch: 300, Train Loss: 0.1436, Train Acc: 95.11, Valid Loss: 0.0485, Valid Acc: 67.59\n",
      "Epoch: 320, Train Loss: 0.1343, Train Acc: 95.45, Valid Loss: 0.0424, Valid Acc: 67.83\n",
      "Epoch: 340, Train Loss: 0.1395, Train Acc: 95.33, Valid Loss: 0.0413, Valid Acc: 67.71\n",
      "Epoch: 360, Train Loss: 0.1172, Train Acc: 95.90, Valid Loss: 0.0298, Valid Acc: 68.35\n",
      "Epoch: 380, Train Loss: 0.1101, Train Acc: 96.23, Valid Loss: 0.0260, Valid Acc: 68.03\n",
      "Epoch: 400, Train Loss: 0.0988, Train Acc: 96.59, Valid Loss: 0.0236, Valid Acc: 67.93\n",
      "Epoch: 420, Train Loss: 0.0912, Train Acc: 96.79, Valid Loss: 0.0201, Valid Acc: 68.01\n",
      "Epoch: 440, Train Loss: 0.0856, Train Acc: 97.14, Valid Loss: 0.0183, Valid Acc: 67.86\n",
      "Epoch: 460, Train Loss: 0.0871, Train Acc: 97.00, Valid Loss: 0.0163, Valid Acc: 67.91\n",
      "Epoch: 480, Train Loss: 0.0837, Train Acc: 97.09, Valid Loss: 0.0149, Valid Acc: 67.46\n",
      "Epoch: 500, Train Loss: 0.0780, Train Acc: 97.25, Valid Loss: 0.0134, Valid Acc: 67.88\n",
      "Epoch: 520, Train Loss: 0.0666, Train Acc: 97.66, Valid Loss: 0.0114, Valid Acc: 67.91\n",
      "Epoch: 540, Train Loss: 0.0686, Train Acc: 97.69, Valid Loss: 0.0094, Valid Acc: 67.49\n",
      "Epoch: 560, Train Loss: 0.0690, Train Acc: 97.73, Valid Loss: 0.0086, Valid Acc: 67.68\n",
      "Epoch: 580, Train Loss: 0.0649, Train Acc: 97.80, Valid Loss: 0.0096, Valid Acc: 67.54\n",
      "Epoch: 600, Train Loss: 0.0589, Train Acc: 97.99, Valid Loss: 0.0099, Valid Acc: 67.41\n",
      "Epoch: 620, Train Loss: 0.0619, Train Acc: 97.76, Valid Loss: 0.0096, Valid Acc: 67.71\n",
      "Epoch: 640, Train Loss: 0.0543, Train Acc: 98.17, Valid Loss: 0.0076, Valid Acc: 67.24\n",
      "Epoch: 660, Train Loss: 0.0533, Train Acc: 98.20, Valid Loss: 0.0070, Valid Acc: 67.73\n",
      "Epoch: 680, Train Loss: 0.0504, Train Acc: 98.17, Valid Loss: 0.0052, Valid Acc: 67.54\n",
      "Epoch: 700, Train Loss: 0.0503, Train Acc: 98.20, Valid Loss: 0.0059, Valid Acc: 67.39\n",
      "Epoch: 720, Train Loss: 0.0531, Train Acc: 98.30, Valid Loss: 0.0053, Valid Acc: 67.71\n",
      "Epoch: 740, Train Loss: 0.0442, Train Acc: 98.53, Valid Loss: 0.0039, Valid Acc: 67.73\n",
      "Epoch: 760, Train Loss: 0.0572, Train Acc: 98.05, Valid Loss: 0.0052, Valid Acc: 67.73\n",
      "Epoch: 780, Train Loss: 0.0375, Train Acc: 98.69, Valid Loss: 0.0032, Valid Acc: 67.83\n",
      "Epoch: 800, Train Loss: 0.0316, Train Acc: 99.04, Valid Loss: 0.0027, Valid Acc: 67.31\n",
      "Epoch: 820, Train Loss: 0.0374, Train Acc: 98.70, Valid Loss: 0.0022, Valid Acc: 67.54\n",
      "Epoch: 840, Train Loss: 0.0373, Train Acc: 98.68, Valid Loss: 0.0032, Valid Acc: 67.14\n",
      "Epoch: 860, Train Loss: 0.0362, Train Acc: 98.70, Valid Loss: 0.0028, Valid Acc: 67.93\n",
      "Epoch: 880, Train Loss: 0.0277, Train Acc: 99.05, Valid Loss: 0.0018, Valid Acc: 68.01\n",
      "Epoch: 900, Train Loss: 0.0305, Train Acc: 98.95, Valid Loss: 0.0017, Valid Acc: 67.51\n",
      "Epoch: 920, Train Loss: 0.0252, Train Acc: 99.11, Valid Loss: 0.0013, Valid Acc: 67.71\n",
      "Epoch: 940, Train Loss: 0.0248, Train Acc: 99.12, Valid Loss: 0.0014, Valid Acc: 67.24\n",
      "Epoch: 960, Train Loss: 0.0252, Train Acc: 99.25, Valid Loss: 0.0013, Valid Acc: 67.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-06-06 18:52:29,348]\u001b[0m Trial 1 failed with parameters: {'vnode': 'bi', 'temperature': 2.574367734217586, 'contrastive_loss_coef': 3.162806300138172e-05, 'cnode_weight': 4.01111551694932, 'lr': 0.003029321416930865, 'positive_sampling': True, 'g_noise': 0.0, 'model': 'sage'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jaewoo/anaconda3/envs/torch_tutorial/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_127185/2395759994.py\", line 52, in objective\n",
      "    val_loss, val_acc = valid_model(model, data_for_tuning, criterion,\n",
      "  File \"/home/jaewoo/DS503_Graph_Class_Imbalance-1/utils.py\", line 54, in valid_model\n",
      "    loss = criterion(out[data.valid_mask], data.y[data.valid_mask])\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-06-06 18:52:29,349]\u001b[0m Trial 1 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_127185/562721671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpruned_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRUNED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tutorial/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_127185/2395759994.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         val_loss, val_acc = valid_model(model, data_for_tuning, criterion, \n\u001b[0m\u001b[1;32m     53\u001b[0m                                         \u001b[0mconstrative_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstrative_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                         \u001b[0mcnode_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnode_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DS503_Graph_Class_Imbalance-1/utils.py\u001b[0m in \u001b[0;36mvalid_model\u001b[0;34m(model, data, criterion, cnode_weight, constrative_coef, temperature, positive_sampling)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpositive_sampling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mCL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConstrativeLosswithPositiveSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'virtualnode' : [UnidirectionalVirtualClassNode(), VirtualClassNode()],\n",
    "    'temperature' : np.linspace(0.1, 1, num=10),\n",
    "    'constrative_coef' : np.logspace(-4, -1, 6),\n",
    "    'lr': np.logspace(-4, -2, 5)\n",
    "}\n",
    "\n",
    "tuning_result = pd.DataFrame({\n",
    "                            'model' : [],\n",
    "                            'virtualnode' : [],\n",
    "                            'temperature' : [],\n",
    "                            'constrative coef' : [],\n",
    "                            'lr' : [],\n",
    "                            'train_acc' : [],\n",
    "                            'train_loss' : [],\n",
    "                            'val_acc' : [],\n",
    "                            'val_loss' : [],\n",
    "                            'test_acc' : [],\n",
    "                            'macro f1' : [],\n",
    "                            'micro f1'\n",
    "                            'minor f1' : [],\n",
    "                            'mad' : [],\n",
    "                            'madgap' : []                            \n",
    "                            })\n",
    "tuning_result.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VC : UnidirectionalVirtualClassNode(), temp : 0.10000, constrative coef : 0.00010, lr : 0.00010 \n",
      "Model: GraphSAGE | Number of parameters: 4853760\n",
      "Epoch: 000, Train Loss: 4.3002, Train Acc: 1.00, Valid Loss: 4.3164, Valid Acc: 0.79\n",
      "Epoch: 010, Train Loss: 4.2717, Train Acc: 4.07, Valid Loss: 4.2959, Valid Acc: 3.92\n",
      "Epoch: 020, Train Loss: 4.2500, Train Acc: 4.24, Valid Loss: 4.2786, Valid Acc: 4.64\n",
      "Epoch: 030, Train Loss: 4.2269, Train Acc: 4.59, Valid Loss: 4.2478, Valid Acc: 4.64\n",
      "Epoch: 040, Train Loss: 4.1707, Train Acc: 4.63, Valid Loss: 4.1765, Valid Acc: 4.27\n",
      "Epoch: 050, Train Loss: 4.0989, Train Acc: 4.31, Valid Loss: 4.1030, Valid Acc: 4.27\n",
      "Epoch: 060, Train Loss: 4.0783, Train Acc: 4.52, Valid Loss: 4.0855, Valid Acc: 4.64\n",
      "Epoch: 070, Train Loss: 4.0666, Train Acc: 4.69, Valid Loss: 4.0743, Valid Acc: 4.64\n",
      "Epoch: 080, Train Loss: 4.0565, Train Acc: 4.93, Valid Loss: 4.0664, Valid Acc: 4.64\n",
      "Epoch: 090, Train Loss: 4.0510, Train Acc: 4.65, Valid Loss: 4.0586, Valid Acc: 4.64\n",
      "Epoch: 100, Train Loss: 4.0374, Train Acc: 4.68, Valid Loss: 4.0477, Valid Acc: 4.64\n",
      "Epoch: 110, Train Loss: 4.0220, Train Acc: 4.91, Valid Loss: 4.0306, Valid Acc: 4.64\n",
      "Epoch: 120, Train Loss: 3.9926, Train Acc: 4.75, Valid Loss: 3.9985, Valid Acc: 4.64\n",
      "Epoch: 130, Train Loss: 3.9281, Train Acc: 4.73, Valid Loss: 3.9311, Valid Acc: 4.82\n",
      "Epoch: 140, Train Loss: 3.8001, Train Acc: 7.20, Valid Loss: 3.7933, Valid Acc: 7.89\n",
      "Epoch: 150, Train Loss: 3.6890, Train Acc: 7.85, Valid Loss: 3.6850, Valid Acc: 9.56\n",
      "Epoch: 160, Train Loss: 3.5962, Train Acc: 10.33, Valid Loss: 3.5937, Valid Acc: 11.29\n",
      "Epoch: 170, Train Loss: 3.4945, Train Acc: 10.80, Valid Loss: 3.4924, Valid Acc: 11.74\n",
      "Epoch: 180, Train Loss: 3.3953, Train Acc: 10.96, Valid Loss: 3.4077, Valid Acc: 11.64\n",
      "Epoch: 190, Train Loss: 3.3341, Train Acc: 11.97, Valid Loss: 3.3437, Valid Acc: 12.71\n",
      "Epoch: 200, Train Loss: 3.2788, Train Acc: 12.16, Valid Loss: 3.2924, Valid Acc: 12.83\n",
      "Epoch: 210, Train Loss: 3.2250, Train Acc: 13.17, Valid Loss: 3.2466, Valid Acc: 13.68\n",
      "Epoch: 220, Train Loss: 3.1779, Train Acc: 13.63, Valid Loss: 3.2038, Valid Acc: 14.47\n",
      "Epoch: 230, Train Loss: 3.1307, Train Acc: 14.64, Valid Loss: 3.1633, Valid Acc: 15.02\n",
      "Epoch: 240, Train Loss: 3.0882, Train Acc: 15.12, Valid Loss: 3.1228, Valid Acc: 16.41\n",
      "Epoch: 250, Train Loss: 3.0497, Train Acc: 16.17, Valid Loss: 3.0802, Valid Acc: 17.85\n",
      "Epoch: 260, Train Loss: 3.0014, Train Acc: 17.73, Valid Loss: 3.0341, Valid Acc: 19.46\n",
      "Epoch: 270, Train Loss: 2.9370, Train Acc: 19.94, Valid Loss: 2.9838, Valid Acc: 21.54\n",
      "Epoch: 280, Train Loss: 2.8892, Train Acc: 20.75, Valid Loss: 2.9288, Valid Acc: 22.54\n",
      "Epoch: 290, Train Loss: 2.8256, Train Acc: 22.32, Valid Loss: 2.8703, Valid Acc: 24.30\n",
      "Epoch: 300, Train Loss: 2.7647, Train Acc: 23.93, Valid Loss: 2.8062, Valid Acc: 25.44\n",
      "Epoch: 310, Train Loss: 2.6962, Train Acc: 25.91, Valid Loss: 2.7399, Valid Acc: 27.77\n",
      "Epoch: 320, Train Loss: 2.6198, Train Acc: 28.29, Valid Loss: 2.6756, Valid Acc: 29.34\n",
      "Epoch: 330, Train Loss: 2.5652, Train Acc: 28.86, Valid Loss: 2.6196, Valid Acc: 30.63\n",
      "Epoch: 340, Train Loss: 2.5107, Train Acc: 30.61, Valid Loss: 2.5711, Valid Acc: 32.17\n",
      "Epoch: 350, Train Loss: 2.4675, Train Acc: 31.97, Valid Loss: 2.5281, Valid Acc: 33.90\n",
      "Epoch: 360, Train Loss: 2.4133, Train Acc: 33.56, Valid Loss: 2.4884, Valid Acc: 35.44\n",
      "Epoch: 370, Train Loss: 2.3834, Train Acc: 33.85, Valid Loss: 2.4504, Valid Acc: 36.41\n",
      "Epoch: 380, Train Loss: 2.3352, Train Acc: 36.32, Valid Loss: 2.4152, Valid Acc: 37.53\n",
      "Epoch: 390, Train Loss: 2.2901, Train Acc: 37.15, Valid Loss: 2.3810, Valid Acc: 39.02\n",
      "Epoch: 400, Train Loss: 2.2576, Train Acc: 38.55, Valid Loss: 2.3459, Valid Acc: 40.23\n",
      "Epoch: 410, Train Loss: 2.2176, Train Acc: 40.03, Valid Loss: 2.3097, Valid Acc: 40.83\n",
      "Epoch: 420, Train Loss: 2.1686, Train Acc: 41.03, Valid Loss: 2.2731, Valid Acc: 42.62\n",
      "Epoch: 430, Train Loss: 2.1149, Train Acc: 42.07, Valid Loss: 2.2262, Valid Acc: 43.56\n",
      "Epoch: 440, Train Loss: 2.0655, Train Acc: 44.45, Valid Loss: 2.1773, Valid Acc: 45.27\n",
      "Epoch: 450, Train Loss: 2.0100, Train Acc: 45.89, Valid Loss: 2.1280, Valid Acc: 46.41\n",
      "Epoch: 460, Train Loss: 1.9541, Train Acc: 47.41, Valid Loss: 2.0777, Valid Acc: 47.75\n",
      "Epoch: 470, Train Loss: 1.8966, Train Acc: 48.41, Valid Loss: 2.0303, Valid Acc: 49.09\n",
      "Epoch: 480, Train Loss: 1.8478, Train Acc: 50.31, Valid Loss: 1.9872, Valid Acc: 50.24\n",
      "Epoch: 490, Train Loss: 1.8019, Train Acc: 51.38, Valid Loss: 1.9487, Valid Acc: 51.35\n",
      "Epoch: 500, Train Loss: 1.7528, Train Acc: 52.72, Valid Loss: 1.9159, Valid Acc: 52.22\n",
      "Epoch: 510, Train Loss: 1.7197, Train Acc: 53.28, Valid Loss: 1.8874, Valid Acc: 53.29\n",
      "Epoch: 520, Train Loss: 1.6838, Train Acc: 54.50, Valid Loss: 1.8616, Valid Acc: 54.03\n",
      "Epoch: 530, Train Loss: 1.6539, Train Acc: 54.71, Valid Loss: 1.8392, Valid Acc: 54.98\n",
      "Epoch: 540, Train Loss: 1.6231, Train Acc: 56.39, Valid Loss: 1.8198, Valid Acc: 55.42\n",
      "Epoch: 550, Train Loss: 1.5946, Train Acc: 56.80, Valid Loss: 1.8026, Valid Acc: 55.92\n",
      "Epoch: 560, Train Loss: 1.5655, Train Acc: 58.47, Valid Loss: 1.7856, Valid Acc: 56.61\n",
      "Epoch: 570, Train Loss: 1.5320, Train Acc: 58.56, Valid Loss: 1.7722, Valid Acc: 57.01\n",
      "Epoch: 580, Train Loss: 1.5244, Train Acc: 59.05, Valid Loss: 1.7572, Valid Acc: 57.61\n",
      "Epoch: 590, Train Loss: 1.4987, Train Acc: 59.08, Valid Loss: 1.7443, Valid Acc: 58.05\n",
      "Epoch: 600, Train Loss: 1.4717, Train Acc: 59.91, Valid Loss: 1.7336, Valid Acc: 58.48\n",
      "Epoch: 610, Train Loss: 1.4602, Train Acc: 60.44, Valid Loss: 1.7233, Valid Acc: 58.80\n",
      "Epoch: 620, Train Loss: 1.4467, Train Acc: 61.40, Valid Loss: 1.7130, Valid Acc: 59.25\n",
      "Epoch: 630, Train Loss: 1.4187, Train Acc: 61.92, Valid Loss: 1.7042, Valid Acc: 59.37\n",
      "Epoch: 640, Train Loss: 1.4046, Train Acc: 61.93, Valid Loss: 1.6963, Valid Acc: 59.84\n",
      "Epoch: 650, Train Loss: 1.3875, Train Acc: 62.44, Valid Loss: 1.6888, Valid Acc: 60.29\n",
      "Epoch: 660, Train Loss: 1.3635, Train Acc: 63.24, Valid Loss: 1.6835, Valid Acc: 60.26\n",
      "Epoch: 670, Train Loss: 1.3504, Train Acc: 63.48, Valid Loss: 1.6771, Valid Acc: 60.64\n",
      "Epoch: 680, Train Loss: 1.3283, Train Acc: 63.91, Valid Loss: 1.6722, Valid Acc: 60.88\n",
      "Epoch: 690, Train Loss: 1.3277, Train Acc: 64.04, Valid Loss: 1.6662, Valid Acc: 61.01\n",
      "Epoch: 700, Train Loss: 1.3080, Train Acc: 64.20, Valid Loss: 1.6622, Valid Acc: 61.16\n",
      "Epoch: 710, Train Loss: 1.3020, Train Acc: 65.01, Valid Loss: 1.6578, Valid Acc: 61.53\n",
      "Epoch: 720, Train Loss: 1.2839, Train Acc: 65.40, Valid Loss: 1.6525, Valid Acc: 61.65\n",
      "Epoch: 730, Train Loss: 1.2720, Train Acc: 65.03, Valid Loss: 1.6508, Valid Acc: 61.53\n",
      "Epoch: 740, Train Loss: 1.2560, Train Acc: 65.68, Valid Loss: 1.6471, Valid Acc: 61.83\n",
      "Epoch: 750, Train Loss: 1.2442, Train Acc: 66.12, Valid Loss: 1.6437, Valid Acc: 62.35\n",
      "Epoch: 760, Train Loss: 1.2423, Train Acc: 65.91, Valid Loss: 1.6392, Valid Acc: 62.32\n",
      "Epoch: 770, Train Loss: 1.2302, Train Acc: 66.42, Valid Loss: 1.6387, Valid Acc: 62.22\n",
      "Epoch: 780, Train Loss: 1.2189, Train Acc: 66.82, Valid Loss: 1.6347, Valid Acc: 62.45\n",
      "Epoch: 790, Train Loss: 1.2099, Train Acc: 67.16, Valid Loss: 1.6344, Valid Acc: 62.87\n",
      "Epoch: 800, Train Loss: 1.2025, Train Acc: 67.06, Valid Loss: 1.6356, Valid Acc: 63.02\n",
      "Epoch: 810, Train Loss: 1.1911, Train Acc: 67.12, Valid Loss: 1.6327, Valid Acc: 63.37\n",
      "Epoch: 820, Train Loss: 1.1836, Train Acc: 67.33, Valid Loss: 1.6309, Valid Acc: 63.09\n",
      "Epoch: 830, Train Loss: 1.1757, Train Acc: 67.38, Valid Loss: 1.6293, Valid Acc: 63.24\n",
      "Epoch: 840, Train Loss: 1.1571, Train Acc: 67.88, Valid Loss: 1.6268, Valid Acc: 63.24\n",
      "Epoch: 850, Train Loss: 1.1618, Train Acc: 67.63, Valid Loss: 1.6233, Valid Acc: 63.37\n",
      "Epoch: 860, Train Loss: 1.1575, Train Acc: 67.86, Valid Loss: 1.6237, Valid Acc: 63.12\n",
      "Epoch: 870, Train Loss: 1.1426, Train Acc: 68.37, Valid Loss: 1.6218, Valid Acc: 63.39\n",
      "Epoch: 880, Train Loss: 1.1370, Train Acc: 69.07, Valid Loss: 1.6233, Valid Acc: 63.24\n",
      "Epoch: 890, Train Loss: 1.1259, Train Acc: 68.47, Valid Loss: 1.6225, Valid Acc: 63.37\n",
      "Epoch: 900, Train Loss: 1.1255, Train Acc: 68.79, Valid Loss: 1.6219, Valid Acc: 63.42\n",
      "Epoch: 910, Train Loss: 1.1187, Train Acc: 68.77, Valid Loss: 1.6188, Valid Acc: 63.39\n",
      "Epoch: 920, Train Loss: 1.1180, Train Acc: 69.13, Valid Loss: 1.6200, Valid Acc: 63.32\n",
      "Epoch: 930, Train Loss: 1.1026, Train Acc: 69.16, Valid Loss: 1.6186, Valid Acc: 63.64\n",
      "Epoch: 940, Train Loss: 1.1091, Train Acc: 69.21, Valid Loss: 1.6175, Valid Acc: 63.74\n",
      "Epoch: 950, Train Loss: 1.1022, Train Acc: 69.44, Valid Loss: 1.6128, Valid Acc: 63.69\n",
      "Epoch: 960, Train Loss: 1.0945, Train Acc: 69.36, Valid Loss: 1.6153, Valid Acc: 63.81\n",
      "Epoch: 970, Train Loss: 1.0765, Train Acc: 69.65, Valid Loss: 1.6148, Valid Acc: 63.89\n",
      "Epoch: 980, Train Loss: 1.0742, Train Acc: 70.15, Valid Loss: 1.6133, Valid Acc: 63.74\n",
      "Epoch: 990, Train Loss: 1.0663, Train Acc: 70.46, Valid Loss: 1.6145, Valid Acc: 63.86\n",
      "Epoch: 1000, Train Loss: 1.0732, Train Acc: 69.89, Valid Loss: 1.6115, Valid Acc: 64.01\n",
      "Epoch: 1010, Train Loss: 1.0447, Train Acc: 70.66, Valid Loss: 1.6094, Valid Acc: 64.18\n",
      "Epoch: 1020, Train Loss: 1.0481, Train Acc: 70.57, Valid Loss: 1.6148, Valid Acc: 64.41\n",
      "Epoch: 1030, Train Loss: 1.0489, Train Acc: 70.63, Valid Loss: 1.6080, Valid Acc: 64.51\n",
      "Epoch: 1040, Train Loss: 1.0368, Train Acc: 70.47, Valid Loss: 1.6098, Valid Acc: 64.83\n",
      "Epoch: 1050, Train Loss: 1.0367, Train Acc: 71.11, Valid Loss: 1.6049, Valid Acc: 64.53\n"
     ]
    }
   ],
   "source": [
    "for virtualnode in hyperparameters['virtualnode']:\n",
    "    if virtualnode is None:\n",
    "        vc = 'None'\n",
    "        data_for_tuning = data\n",
    "        constrative_flag = False\n",
    "    else:\n",
    "        vc = virtualnode\n",
    "        data_for_tuning = vc.forward(data)\n",
    "        constrative_flag = True\n",
    "        \n",
    "    for temperature in hyperparameters['temperature']:\n",
    "        for constrative_coef in hyperparameters['constrative_coef']:\n",
    "            for lr in hyperparameters['lr']:\n",
    "                models = [GraphSAGE(in_channels=dataset.num_features, hidden_channels=256, number_of_classes=dataset.num_classes, num_of_hidden_layers=4, device=device)]\n",
    "                for model in models:\n",
    "                    print(f'VC : {vc}, temp : {temperature:.5f}, constrative coef : {constrative_coef:.5f}, lr : {lr:.5f} ')\n",
    "                    max_loss = 10000\n",
    "                    early_stopping_count = 0\n",
    "                    print(f'Model: {model.name} | Number of parameters: {model.get_n_params()}')\n",
    "                    model = model.to(device)\n",
    "                    data_for_tuning = data_for_tuning.to(device)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    val_losses = []\n",
    "                    val_accs = []\n",
    "                    for epoch in range(5000):\n",
    "                        loss, acc = train_constrative_model(model, data_for_tuning, optimizer, criterion, \n",
    "                                                            constrative_coef=constrative_coef, temperature=temperature)\n",
    "                        losses.append(loss.item())\n",
    "                        accs.append(100*acc)\n",
    "                        val_loss, val_acc = valid_model(model, data_for_tuning, criterion, constrative_flag=constrative_flag, \n",
    "                                                        constrative_coef=constrative_coef, temperature=temperature)\n",
    "                        val_accs.append(100*val_acc)\n",
    "                        if val_loss < max_loss:\n",
    "                            max_loss = val_loss\n",
    "                            early_stopping_count = 0\n",
    "                        else:\n",
    "                            early_stopping_count += 1\n",
    "                            if early_stopping_count > EARLY_STOPPING:\n",
    "                                print(\"Early stopping..\")\n",
    "                                break\n",
    "                        if epoch%10==0:\n",
    "                            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train Acc: {100*acc:.2f}, Valid Loss: {val_loss:.4f}, Valid Acc: {100*val_acc:.2f}')\n",
    "                        if epoch > 200:\n",
    "                            if val_acc < 0.1:\n",
    "                                print('underfitting...')\n",
    "                                break\n",
    "                    report = test_model(model, data_for_tuning)\n",
    "                    result = pd.DataFrame(report).T\n",
    "                    result_sliced = result.iloc[:-3 if len(result) < 23 else 20, :]\n",
    "                    test_acc = result.loc['accuracy'][0]\n",
    "                    result.loc['minorities-f1',:] = result_sliced.mean(axis=0)\n",
    "                    result.to_csv(os.path.join(SAVE_PATH, f'{model.name}_layers{model.num_of_hidden_layers}_neurons{model.hidden_channels}'+'.csv'))\n",
    "                    result = model(data_for_tuning.x.to(device), data_for_tuning.edge_index.to(device))[0].cpu()\n",
    "                    global_mad = mad(result).item()\n",
    "                    mad_gap = madgap(result, data_for_tuning.edge_index).item()\n",
    "                    \n",
    "                    exp_result_dict = {\n",
    "                        'model' : model.name,\n",
    "                        'virtualnode' : vc,\n",
    "                        'temperature' : temperature,\n",
    "                        'constrative coef' : constrative_coef,\n",
    "                        'lr' : lr,\n",
    "                        'train_acc' : acc,\n",
    "                        'train_loss' : loss,\n",
    "                        'val_acc' : val_acc,\n",
    "                        'val_loss' : val_loss,\n",
    "                        'test_acc' : test_acc,\n",
    "                        'macro f1' : pd.DataFrame(report).T.loc['macro avg', 'f1-score'],\n",
    "                        'micro f1' : pd.DataFrame(report).T.loc['weighted avg', 'f1-score'],\n",
    "                        'minor f1' : pd.DataFrame(report).T[:-3].sort_values(by='support', ascending=False)[-11:].mean()['f1-score'],\n",
    "                        'mad' : global_mad,\n",
    "                        'madgap' : mad_gap                            \n",
    "                    }\n",
    "                    \n",
    "                    tuning_result = tuning_result.append(exp_result_dict, ignore_index=True)\n",
    "                    \n",
    "                    print(f'global_mad: {global_mad}')\n",
    "                    print(f'madgap: {mad_gap}')\n",
    "                    print(f'Test Acc: {100*test_acc}')\n",
    "                    \n",
    "                    print('==========================================', end='\\n\\n')\n",
    "                    del model\n",
    "                    torch.cuda.empty_cache()   \n",
    "                    \n",
    "    del data_for_tuning     \n",
    "    torch.cuda.empty_cache()        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
