{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 0. setting"
      ],
      "metadata": {
        "id": "zDSABMpOSzMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variable names\n",
        "\n",
        "# cora : original dataset\n",
        "# cora_X : input X (torch_dataset like format)\n",
        "# cora_X_train : masking for train (50%)\n",
        "# cora_X_val : masking for validation (25%)\n",
        "# cora_X_test : masking for test (25%)\n",
        "# cora_Y : node class label Y\n",
        "# cora_edge : edge_index (torch_dataset like format)"
      ],
      "metadata": {
        "id": "RJDrZrGP_NYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05rpoSTjSle-",
        "outputId": "1e6c9745-3bc7-46c0-c13f-ef309d4e2f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## Node Classification colab guide\n",
        "\n",
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "## Hyeongchan's setting\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "myroute = '/content/drive/MyDrive/graph'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. data load"
      ],
      "metadata": {
        "id": "19Mt00LmTRdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Graph2Gauss's util.py (corafull 제공)\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def load_dataset(file_name):\n",
        "    \"\"\"Load a graph from a Numpy binary file.\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : str\n",
        "        Name of the file to load.\n",
        "    Returns\n",
        "    -------\n",
        "    graph : dict\n",
        "        Dictionary that contains:\n",
        "            * 'A' : The adjacency matrix in sparse matrix format\n",
        "            * 'X' : The attribute matrix in sparse matrix format\n",
        "            * 'z' : The ground truth class labels\n",
        "            * Further dictionaries mapping node, class and attribute IDs\n",
        "    \"\"\"\n",
        "    if not file_name.endswith('.npz'):\n",
        "        file_name += '.npz'\n",
        "    with np.load(file_name, allow_pickle = True) as loader:\n",
        "        loader = dict(loader)\n",
        "        A = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
        "                           loader['adj_indptr']), shape=loader['adj_shape'])\n",
        "\n",
        "        X = sp.csr_matrix((loader['attr_data'], loader['attr_indices'],\n",
        "                           loader['attr_indptr']), shape=loader['attr_shape'])\n",
        "\n",
        "        z = loader.get('labels')\n",
        "\n",
        "        graph = {\n",
        "            'A': A,\n",
        "            'X': X,\n",
        "            'z': z\n",
        "        }\n",
        "\n",
        "        idx_to_node = loader.get('idx_to_node')\n",
        "        if idx_to_node:\n",
        "            idx_to_node = idx_to_node.tolist()\n",
        "            graph['idx_to_node'] = idx_to_node\n",
        "\n",
        "        idx_to_attr = loader.get('idx_to_attr')\n",
        "        if idx_to_attr:\n",
        "            idx_to_attr = idx_to_attr.tolist()\n",
        "            graph['idx_to_attr'] = idx_to_attr\n",
        "\n",
        "        idx_to_class = loader.get('idx_to_class')\n",
        "        if idx_to_class:\n",
        "            idx_to_class = idx_to_class.tolist()\n",
        "            graph['idx_to_class'] = idx_to_class\n",
        "\n",
        "        return graph\n",
        "\n",
        "\n",
        "## Hyeongchan's setting\n",
        "\n",
        "cora = load_dataset(myroute + '/corafull/cora.npz')\n",
        "\n",
        "print(len(cora['idx_to_node'])) # nodes = 19793 (paper)\n",
        "print(len(cora['idx_to_attr'])) # features = 8710 (bag of words)\n",
        "print(len(cora['idx_to_class'])) # class = 70 (category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6JSUV_ljVhl",
        "outputId": "dc149107-e080-454f-a52b-5680f066cb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19793\n",
            "8710\n",
            "70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. model"
      ],
      "metadata": {
        "id": "iF4ZCyz8oMl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-1. MLP"
      ],
      "metadata": {
        "id": "UqC-BDnQoMi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Hyeongchan's setting\n",
        "\n",
        "cora_num_features = len(cora['idx_to_attr'])\n",
        "cora_num_classes = len(cora['idx_to_class'])\n",
        "\n",
        "\n",
        "## Node Classification colab guide\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = Linear(cora_num_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, cora_num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OaS3qtYvoMbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Hyeongchan's setting\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# sparse matrix X -> torch\n",
        "\n",
        "temp = cora['X'].tocoo()\n",
        "cora_X = torch.sparse.LongTensor(torch.LongTensor([temp.row.tolist(), temp.col.tolist()]),\n",
        "                                 torch.LongTensor(temp.data.astype(np.int32))).to_dense().float()\n",
        "\n",
        "# mask index\n",
        "\n",
        "temp = np.arange(len(cora_X))\n",
        "np.random.shuffle(temp)\n",
        "\n",
        "train_len = int(len(cora_X) * 0.5)\n",
        "val_len = (len(cora_X) - train_len) // 2\n",
        "test_len = len(cora_X) - train_len - val_len\n",
        "\n",
        "print(train_len) # 50%\n",
        "print(val_len) # 25%\n",
        "print(test_len) # 25%\n",
        "\n",
        "cora_X_train = temp[ : train_len]\n",
        "cora_X_val = temp[train_len : train_len + val_len]\n",
        "cora_X_test = temp[train_len + val_len : ]\n",
        "\n",
        "print(cora_X_train) # train mask\n",
        "print(cora_X_val) # validation mask\n",
        "print(cora_X_test) # test mask\n",
        "\n",
        "# label\n",
        "\n",
        "cora_Y = torch.tensor(cora['z'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBLwcoSosyKr",
        "outputId": "1a9bc756-0a7e-477b-c835-2cc7430431d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9896\n",
            "4948\n",
            "4949\n",
            "[ 2538  6961  4425 ...  2877 14371  8803]\n",
            "[17972 19701 15026 ...   345 16880  6781]\n",
            "[12459  4962  6205 ... 13680 14958 18552]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Node Classification colab guide\n",
        "\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(cora_X)  # Perform a single forward pass.\n",
        "      loss = criterion(out[cora_X_train], cora_Y[cora_X_train])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(cora_X)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[cora_X_test] == cora_Y[cora_X_test]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / len(cora_X_test)  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 101): # 200 -> 100번으로 줄임\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "7L9TZdrApray",
        "outputId": "6fbaa962-4724-48d1-b4de-0c0b09b0cbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 4.2562\n",
            "Epoch: 002, Loss: 4.2492\n",
            "Epoch: 003, Loss: 4.2424\n",
            "Epoch: 004, Loss: 4.2352\n",
            "Epoch: 005, Loss: 4.2281\n",
            "Epoch: 006, Loss: 4.2200\n",
            "Epoch: 007, Loss: 4.2113\n",
            "Epoch: 008, Loss: 4.2029\n",
            "Epoch: 009, Loss: 4.1938\n",
            "Epoch: 010, Loss: 4.1851\n",
            "Epoch: 011, Loss: 4.1751\n",
            "Epoch: 012, Loss: 4.1655\n",
            "Epoch: 013, Loss: 4.1548\n",
            "Epoch: 014, Loss: 4.1452\n",
            "Epoch: 015, Loss: 4.1354\n",
            "Epoch: 016, Loss: 4.1253\n",
            "Epoch: 017, Loss: 4.1167\n",
            "Epoch: 018, Loss: 4.1053\n",
            "Epoch: 019, Loss: 4.0975\n",
            "Epoch: 020, Loss: 4.0896\n",
            "Epoch: 021, Loss: 4.0812\n",
            "Epoch: 022, Loss: 4.0757\n",
            "Epoch: 023, Loss: 4.0656\n",
            "Epoch: 024, Loss: 4.0579\n",
            "Epoch: 025, Loss: 4.0569\n",
            "Epoch: 026, Loss: 4.0486\n",
            "Epoch: 027, Loss: 4.0469\n",
            "Epoch: 028, Loss: 4.0459\n",
            "Epoch: 029, Loss: 4.0417\n",
            "Epoch: 030, Loss: 4.0394\n",
            "Epoch: 031, Loss: 4.0379\n",
            "Epoch: 032, Loss: 4.0372\n",
            "Epoch: 033, Loss: 4.0365\n",
            "Epoch: 034, Loss: 4.0358\n",
            "Epoch: 035, Loss: 4.0365\n",
            "Epoch: 036, Loss: 4.0330\n",
            "Epoch: 037, Loss: 4.0357\n",
            "Epoch: 038, Loss: 4.0318\n",
            "Epoch: 039, Loss: 4.0343\n",
            "Epoch: 040, Loss: 4.0324\n",
            "Epoch: 041, Loss: 4.0309\n",
            "Epoch: 042, Loss: 4.0295\n",
            "Epoch: 043, Loss: 4.0300\n",
            "Epoch: 044, Loss: 4.0256\n",
            "Epoch: 045, Loss: 4.0285\n",
            "Epoch: 046, Loss: 4.0268\n",
            "Epoch: 047, Loss: 4.0270\n",
            "Epoch: 048, Loss: 4.0286\n",
            "Epoch: 049, Loss: 4.0281\n",
            "Epoch: 050, Loss: 4.0224\n",
            "Epoch: 051, Loss: 4.0256\n",
            "Epoch: 052, Loss: 4.0251\n",
            "Epoch: 053, Loss: 4.0248\n",
            "Epoch: 054, Loss: 4.0266\n",
            "Epoch: 055, Loss: 4.0259\n",
            "Epoch: 056, Loss: 4.0251\n",
            "Epoch: 057, Loss: 4.0266\n",
            "Epoch: 058, Loss: 4.0257\n",
            "Epoch: 059, Loss: 4.0233\n",
            "Epoch: 060, Loss: 4.0245\n",
            "Epoch: 061, Loss: 4.0252\n",
            "Epoch: 062, Loss: 4.0240\n",
            "Epoch: 063, Loss: 4.0233\n",
            "Epoch: 064, Loss: 4.0235\n",
            "Epoch: 065, Loss: 4.0232\n",
            "Epoch: 066, Loss: 4.0229\n",
            "Epoch: 067, Loss: 4.0256\n",
            "Epoch: 068, Loss: 4.0241\n",
            "Epoch: 069, Loss: 4.0245\n",
            "Epoch: 070, Loss: 4.0224\n",
            "Epoch: 071, Loss: 4.0237\n",
            "Epoch: 072, Loss: 4.0243\n",
            "Epoch: 073, Loss: 4.0222\n",
            "Epoch: 074, Loss: 4.0211\n",
            "Epoch: 075, Loss: 4.0219\n",
            "Epoch: 076, Loss: 4.0260\n",
            "Epoch: 077, Loss: 4.0208\n",
            "Epoch: 078, Loss: 4.0234\n",
            "Epoch: 079, Loss: 4.0232\n",
            "Epoch: 080, Loss: 4.0218\n",
            "Epoch: 081, Loss: 4.0213\n",
            "Epoch: 082, Loss: 4.0194\n",
            "Epoch: 083, Loss: 4.0243\n",
            "Epoch: 084, Loss: 4.0211\n",
            "Epoch: 085, Loss: 4.0217\n",
            "Epoch: 086, Loss: 4.0226\n",
            "Epoch: 087, Loss: 4.0244\n",
            "Epoch: 088, Loss: 4.0177\n",
            "Epoch: 089, Loss: 4.0223\n",
            "Epoch: 090, Loss: 4.0204\n",
            "Epoch: 091, Loss: 4.0197\n",
            "Epoch: 092, Loss: 4.0209\n",
            "Epoch: 093, Loss: 4.0211\n",
            "Epoch: 094, Loss: 4.0225\n",
            "Epoch: 095, Loss: 4.0222\n",
            "Epoch: 096, Loss: 4.0213\n",
            "Epoch: 097, Loss: 4.0218\n",
            "Epoch: 098, Loss: 4.0225\n",
            "Epoch: 099, Loss: 4.0233\n",
            "Epoch: 100, Loss: 4.0190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}') # 0.05 정도, 원래 이런가? -> 70개라 못 맞추는 건가?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGsW37fkpyJ9",
        "outputId": "d8f58f94-27c6-45be-d6fc-f69c28cb34df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-2. Simple GCN"
      ],
      "metadata": {
        "id": "1y_Tk2Ds_vlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Node Classification colab guide\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(cora_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, cora_num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FmzNeVjd_vjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Hyeongchan's setting\n",
        "\n",
        "cora_edge = torch.tensor(cora['A'].nonzero()).contiguous() # adjacency matrix -> edge index\n",
        "\n",
        "cora_edge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y06BZLRQEYOk",
        "outputId": "8d88c3d5-bac4-4d46-b7e7-5dbd7172fe29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,     0,     0,  ..., 19791, 19791, 19791],\n",
              "        [ 1227,  4021,  4105,  ...,  2099,  5100, 10850]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Node Classification colab guide\n",
        "\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(cora_X, cora_edge)  # Perform a single forward pass.\n",
        "      loss = criterion(out[cora_X_train], cora_Y[cora_X_train])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(cora_X, cora_edge)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[cora_X_test] == cora_Y[cora_X_test]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / len(cora_X_test)  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "Umr6O4Ur_vgo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "556b1c6c-9187-4fdc-dd55-06fe6fc1b8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 4.2485\n",
            "Epoch: 002, Loss: 4.2429\n",
            "Epoch: 003, Loss: 4.2375\n",
            "Epoch: 004, Loss: 4.2321\n",
            "Epoch: 005, Loss: 4.2269\n",
            "Epoch: 006, Loss: 4.2217\n",
            "Epoch: 007, Loss: 4.2167\n",
            "Epoch: 008, Loss: 4.2117\n",
            "Epoch: 009, Loss: 4.2068\n",
            "Epoch: 010, Loss: 4.2021\n",
            "Epoch: 011, Loss: 4.1974\n",
            "Epoch: 012, Loss: 4.1928\n",
            "Epoch: 013, Loss: 4.1884\n",
            "Epoch: 014, Loss: 4.1840\n",
            "Epoch: 015, Loss: 4.1797\n",
            "Epoch: 016, Loss: 4.1755\n",
            "Epoch: 017, Loss: 4.1714\n",
            "Epoch: 018, Loss: 4.1674\n",
            "Epoch: 019, Loss: 4.1635\n",
            "Epoch: 020, Loss: 4.1597\n",
            "Epoch: 021, Loss: 4.1560\n",
            "Epoch: 022, Loss: 4.1524\n",
            "Epoch: 023, Loss: 4.1488\n",
            "Epoch: 024, Loss: 4.1454\n",
            "Epoch: 025, Loss: 4.1420\n",
            "Epoch: 026, Loss: 4.1387\n",
            "Epoch: 027, Loss: 4.1355\n",
            "Epoch: 028, Loss: 4.1323\n",
            "Epoch: 029, Loss: 4.1292\n",
            "Epoch: 030, Loss: 4.1262\n",
            "Epoch: 031, Loss: 4.1233\n",
            "Epoch: 032, Loss: 4.1204\n",
            "Epoch: 033, Loss: 4.1176\n",
            "Epoch: 034, Loss: 4.1149\n",
            "Epoch: 035, Loss: 4.1123\n",
            "Epoch: 036, Loss: 4.1097\n",
            "Epoch: 037, Loss: 4.1071\n",
            "Epoch: 038, Loss: 4.1047\n",
            "Epoch: 039, Loss: 4.1023\n",
            "Epoch: 040, Loss: 4.0999\n",
            "Epoch: 041, Loss: 4.0976\n",
            "Epoch: 042, Loss: 4.0954\n",
            "Epoch: 043, Loss: 4.0933\n",
            "Epoch: 044, Loss: 4.0911\n",
            "Epoch: 045, Loss: 4.0891\n",
            "Epoch: 046, Loss: 4.0871\n",
            "Epoch: 047, Loss: 4.0851\n",
            "Epoch: 048, Loss: 4.0832\n",
            "Epoch: 049, Loss: 4.0814\n",
            "Epoch: 050, Loss: 4.0795\n",
            "Epoch: 051, Loss: 4.0778\n",
            "Epoch: 052, Loss: 4.0761\n",
            "Epoch: 053, Loss: 4.0744\n",
            "Epoch: 054, Loss: 4.0728\n",
            "Epoch: 055, Loss: 4.0712\n",
            "Epoch: 056, Loss: 4.0697\n",
            "Epoch: 057, Loss: 4.0682\n",
            "Epoch: 058, Loss: 4.0667\n",
            "Epoch: 059, Loss: 4.0653\n",
            "Epoch: 060, Loss: 4.0639\n",
            "Epoch: 061, Loss: 4.0626\n",
            "Epoch: 062, Loss: 4.0613\n",
            "Epoch: 063, Loss: 4.0600\n",
            "Epoch: 064, Loss: 4.0587\n",
            "Epoch: 065, Loss: 4.0575\n",
            "Epoch: 066, Loss: 4.0564\n",
            "Epoch: 067, Loss: 4.0552\n",
            "Epoch: 068, Loss: 4.0541\n",
            "Epoch: 069, Loss: 4.0530\n",
            "Epoch: 070, Loss: 4.0520\n",
            "Epoch: 071, Loss: 4.0510\n",
            "Epoch: 072, Loss: 4.0500\n",
            "Epoch: 073, Loss: 4.0490\n",
            "Epoch: 074, Loss: 4.0481\n",
            "Epoch: 075, Loss: 4.0472\n",
            "Epoch: 076, Loss: 4.0463\n",
            "Epoch: 077, Loss: 4.0454\n",
            "Epoch: 078, Loss: 4.0446\n",
            "Epoch: 079, Loss: 4.0437\n",
            "Epoch: 080, Loss: 4.0429\n",
            "Epoch: 081, Loss: 4.0422\n",
            "Epoch: 082, Loss: 4.0414\n",
            "Epoch: 083, Loss: 4.0407\n",
            "Epoch: 084, Loss: 4.0400\n",
            "Epoch: 085, Loss: 4.0393\n",
            "Epoch: 086, Loss: 4.0386\n",
            "Epoch: 087, Loss: 4.0379\n",
            "Epoch: 088, Loss: 4.0373\n",
            "Epoch: 089, Loss: 4.0367\n",
            "Epoch: 090, Loss: 4.0361\n",
            "Epoch: 091, Loss: 4.0355\n",
            "Epoch: 092, Loss: 4.0349\n",
            "Epoch: 093, Loss: 4.0344\n",
            "Epoch: 094, Loss: 4.0338\n",
            "Epoch: 095, Loss: 4.0333\n",
            "Epoch: 096, Loss: 4.0328\n",
            "Epoch: 097, Loss: 4.0323\n",
            "Epoch: 098, Loss: 4.0318\n",
            "Epoch: 099, Loss: 4.0313\n",
            "Epoch: 100, Loss: 4.0309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}') # 똑같이 나오네?... -> 내가 잘못했나봄"
      ],
      "metadata": {
        "id": "dUutmIxC_veE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a54e716-9a55-4c3b-d188-0e9c40403ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-3. GraphSAGE, GAT, GIN...?"
      ],
      "metadata": {
        "id": "4IjEb0o4_vbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3-GMFxM_vSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 99. others"
      ],
      "metadata": {
        "id": "piNiYxea9clC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# masking 없이 바로 자르기\n",
        "\n",
        "# temp, cora_X_test = train_test_split(cora_X, test_size = 0.25)\n",
        "# cora_X_train, cora_X_val = train_test_split(temp, test_size = 1/3)\n",
        "\n",
        "# print(len(cora_X_train)) # train 50% = 9896\n",
        "# print(len(cora_X_val)) # val 25% = 4948\n",
        "# print(len(cora_X_test)) # test 25% = 4949"
      ],
      "metadata": {
        "id": "gOZXxgkT1H20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GraphSMOTE's data_load.py (아마도 normal cora)\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "# import utils\n",
        "\n",
        "def load_data(path= 'data/cora/', dataset=\"cora\"):#modified from code: pygcn\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    #input: idx_features_labels, adj\n",
        "    #idx,labels are not required to be processed in advance\n",
        "    #adj: save in the form of edges. idx1 idx2 \n",
        "    #output: adj, features, labels are all torch.tensor, in the dense form\n",
        "    #-------------------------------------------------------\n",
        "\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = idx_features_labels[:, -1]\n",
        "    set_labels = set(labels)\n",
        "    classes_dict = {c: np.arange(len(set_labels))[i] for i, c in enumerate(set_labels)}\n",
        "    classes_dict = {'Neural_Networks': 0, 'Reinforcement_Learning': 1, 'Probabilistic_Methods': 2, 'Case_Based': 3, 'Theory': 4, 'Rule_Learning': 5, 'Genetic_Algorithms': 6}\n",
        "\n",
        "    #ipdb.set_trace()\n",
        "    labels = np.array(list(map(classes_dict.get, labels)))\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    # utils.print_edges_num(adj.todense(), labels)\n",
        "\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    #adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    return adj, features, labels\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "x9B3_dTwcDzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_data()"
      ],
      "metadata": {
        "id": "F8VV1S3oedtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5feec7de-5543-4900-f86d-898f9116396e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
              "                        [   8,   14,  258,  ...,  774, 1389, 2344]]),\n",
              "        values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
              "        size=(2708, 2708), nnz=10556, layout=torch.sparse_coo),\n",
              " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([0, 5, 1,  ..., 6, 3, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}